{"id":"mcpIndexer-107","title":"Optimize batch sizes for embeddings and ChromaDB","description":"**Goal**: Tune batch sizes for both embedding generation and ChromaDB insertions to maximize throughput.\n\n**What we're accomplishing**: Current batch_size=1000 may not be optimal. Different phases have different optimal batch sizes:\n- Embedding generation: 128-512 (depends on GPU memory)\n- ChromaDB insertions: 5000+ (database write efficiency)\n\n**Impact**: 10-30% improvement in indexing throughput with no code complexity.\n\n**Implementation**:\n- Separate EMBEDDING_BATCH_SIZE and DB_BATCH_SIZE constants\n- Benchmark different sizes on test repos\n- Adjust defaults based on hardware (GPU vs CPU)\n- Make configurable via environment variables\n- Document performance characteristics","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-27T18:13:44.768771-05:00","updated_at":"2025-10-27T20:19:10.369471-05:00","closed_at":"2025-10-27T20:19:10.369471-05:00"}
{"id":"mcpIndexer-114","title":"Implement GPU auto-detection logic","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-27T18:15:57.375444-05:00","updated_at":"2025-10-27T20:28:11.709759-05:00","closed_at":"2025-10-27T20:28:11.709759-05:00","dependencies":[{"issue_id":"mcpIndexer-114","depends_on_id":"mcpIndexer-105","type":"blocks","created_at":"2025-10-27T18:15:57.377014-05:00","created_by":"gkatechis"}]}
{"id":"mcpIndexer-115","title":"Add CPU/GPU configuration option and fallback","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-27T18:16:03.066348-05:00","updated_at":"2025-10-27T20:28:24.060699-05:00","closed_at":"2025-10-27T20:28:24.060699-05:00","dependencies":[{"issue_id":"mcpIndexer-115","depends_on_id":"mcpIndexer-105","type":"blocks","created_at":"2025-10-27T18:16:03.067677-05:00","created_by":"gkatechis"}]}
{"id":"mcpIndexer-116","title":"Document GPU setup and requirements","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-27T18:16:08.83076-05:00","updated_at":"2025-10-27T20:28:36.07549-05:00","closed_at":"2025-10-27T20:28:36.07549-05:00","dependencies":[{"issue_id":"mcpIndexer-116","depends_on_id":"mcpIndexer-105","type":"blocks","created_at":"2025-10-27T18:16:08.832258-05:00","created_by":"gkatechis"}]}
{"id":"mcpIndexer-126","title":"Add GPU acceleration support for embeddings","description":"**Goal**: Enable GPU acceleration for embedding generation to dramatically improve indexing speed.\n\n**What we're accomplishing**: Sentence-transformers supports CUDA/GPU acceleration which provides 10-50x speedup:\n- CPU: 170-750 queries/sec\n- GPU: 4,000-18,000 queries/sec\n\n**Impact**: Indexing large repositories becomes 10-50x faster. A repo that takes 10 minutes could index in 12-60 seconds.\n\n**Implementation**:\n- Auto-detect GPU availability (torch.cuda.is_available())\n- Add device parameter to SentenceTransformer initialization\n- Add configuration option to force CPU/GPU\n- Update documentation with GPU setup instructions\n- Gracefully fallback to CPU if GPU unavailable","status":"in_progress","priority":1,"issue_type":"feature","created_at":"2025-10-27T20:33:48.924899-05:00","updated_at":"2025-10-27T20:33:48.924899-05:00"}
{"id":"mcpIndexer-99","title":"Fix prerequisites section formatting in README","description":"","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-10-27T16:44:21.130417-05:00","updated_at":"2025-10-27T20:27:58.740777-05:00","closed_at":"2025-10-27T20:27:58.740777-05:00"}
