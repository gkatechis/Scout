{"id":"mcpIndexer-105","title":"Add GPU acceleration support for embeddings","description":"**Goal**: Enable GPU acceleration for embedding generation to dramatically improve indexing speed.\n\n**What we're accomplishing**: Sentence-transformers supports CUDA/GPU acceleration which provides 10-50x speedup:\n- CPU: 170-750 queries/sec\n- GPU: 4,000-18,000 queries/sec\n\n**Impact**: Indexing large repositories becomes 10-50x faster. A repo that takes 10 minutes could index in 12-60 seconds.\n\n**Implementation**:\n- Auto-detect GPU availability (torch.cuda.is_available())\n- Add device parameter to SentenceTransformer initialization\n- Add configuration option to force CPU/GPU\n- Update documentation with GPU setup instructions\n- Gracefully fallback to CPU if GPU unavailable","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-10-27T18:13:33.834705-05:00","updated_at":"2025-10-27T20:11:02.386648-05:00","closed_at":"2025-10-27T20:11:02.386648-05:00"}
{"id":"mcpIndexer-107","title":"Optimize batch sizes for embeddings and ChromaDB","description":"**Goal**: Tune batch sizes for both embedding generation and ChromaDB insertions to maximize throughput.\n\n**What we're accomplishing**: Current batch_size=1000 may not be optimal. Different phases have different optimal batch sizes:\n- Embedding generation: 128-512 (depends on GPU memory)\n- ChromaDB insertions: 5000+ (database write efficiency)\n\n**Impact**: 10-30% improvement in indexing throughput with no code complexity.\n\n**Implementation**:\n- Separate EMBEDDING_BATCH_SIZE and DB_BATCH_SIZE constants\n- Benchmark different sizes on test repos\n- Adjust defaults based on hardware (GPU vs CPU)\n- Make configurable via environment variables\n- Document performance characteristics","status":"in_progress","priority":2,"issue_type":"task","created_at":"2025-10-27T18:13:44.768771-05:00","updated_at":"2025-10-27T20:11:48.917611-05:00"}
