{"id":"mcpIndexer-129","title":"Research: Optimize indexing performance - analyze bottlenecks and solutions","description":"**Goal**: Research and identify viable strategies to improve indexing performance.\n\n**Background**: \nAttempted parallel file processing (issue #106) but benchmarking revealed:\n- Parsing/chunking is only ~20% of indexing time\n- Embedding generation is ~70% of indexing time (the bottleneck)\n- Parallel processing of parsing actually made things 52-72% SLOWER due to multiprocessing overhead\n\n**Current Performance**:\n- Small repo (22 files): ~7.4 seconds (sequential)\n- Large repo (11,229 files): ~1,078 seconds (~18 minutes)\n- Throughput: 10-11 files/sec\n\n**RESEARCH FINDINGS**:\n===================\n\n**1. GPU Acceleration (HIGHEST IMPACT)**\n- Expected speedup: 20-24x\n- Evidence: sentence-transformers benchmarks show MiniLM-L6 models go from 750 queries/sec (CPU) to 18,000 queries/sec (GPU)\n- Hardware: NVIDIA GPU with CUDA, or Apple Silicon with MPS\n- Implementation: Simple device parameter change\n- Source: sentence-transformers documentation\n\n**2. Built-in Multi-Process Pool (HIGH IMPACT)**\n- Expected speedup: 2-4x with multiple devices\n- Key insight: sentence-transformers has built-in start_multi_process_pool() that parallelizes EMBEDDING (not parsing)\n- This targets the 70% bottleneck, unlike our failed approach\n- Works with multiple GPUs or CPUs\n- Source: sentence-transformers source code\n\n**3. Batch Size Optimization (MEDIUM IMPACT)**\n- Expected speedup: 1.2-2x\n- Current: default batch_size=32\n- Test: 64, 128, 256, 512\n- Zero code complexity\n- Source: sentence-transformers encode() docs\n\n**4. Quantization (MEDIUM IMPACT)**\n- Expected speedup: 2-3x\n- Options: int8, uint8, binary embeddings\n- Tradeoff: Speed vs quality\n- Source: sentence-transformers encode() precision parameter\n\n**5. Model Selection (LOW IMPACT)**\n- Current model (all-MiniLM-L6-v2) already fastest in class\n- Marginal gains from switching models\n\n**6. ChromaDB Optimization (LOW IMPACT)**\n- Limited documentation found\n- Current batch_size=1000 likely adequate\n- Not the bottleneck\n\n**RECOMMENDED TESTING ORDER**:\n1. GPU acceleration (Phase 1) - could solve problem alone\n2. Multi-process pool (Phase 2) - if multiple devices available\n3. Batch size + quantization (Phase 3) - easy wins\n\n**KEY INSIGHT**:\nOur previous parallel processing attempt failed because we parallelized parsing (20% of work). \nsentence-transformers' built-in pool parallelizes embedding (70% of work) - exactly what we need.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-28T14:59:09.979433-05:00","updated_at":"2025-10-28T15:30:30.989966-05:00","closed_at":"2025-10-28T15:30:30.989966-05:00"}
{"id":"mcpIndexer-130","title":"Improve keyword search tokenization for code syntax","description":"Current keyword search tokenization uses simple whitespace splitting, which doesn't work well for code syntax. This causes BM25 to miss exact matches.\n\nPROBLEM FROM BENCHMARK:\n- Query: 'authenticate_user' → No keyword results found\n- Query: 'hash_password bcrypt' → No keyword results found\n- Simple split() doesn't handle code identifiers properly\n\nIMPROVEMENTS NEEDED:\n1. Snake_case handling\n   - Split 'authenticate_user' → ['authenticate', 'user']\n   - Split 'get_user_from_database' → ['get', 'user', 'from', 'database']\n\n2. CamelCase handling\n   - Split 'getUserName' → ['get', 'user', 'name']\n   - Split 'AuthService' → ['auth', 'service']\n\n3. Special character tokenization\n   - Split 'hash_password(' → ['hash', 'password']\n   - Split 'user.authenticate()' → ['user', 'authenticate']\n   - Handle dots, parens, brackets, etc.\n\n4. Preserve original tokens\n   - Keep 'authenticate_user' as single token\n   - Also add split versions: 'authenticate', 'user'\n   - This gives both exact and partial matching\n\nIMPLEMENTATION APPROACH:\n- Modify KeywordSearchIndex.index_chunks() tokenization\n- Create custom tokenizer function: tokenize_code()\n- Use regex to split on: underscores, camelCase, special chars\n- Apply to both indexing and query tokenization\n\nEXPECTED RESULTS:\n- Exact function names found by keyword search\n- Parts of compound names also searchable\n- Better hybrid search results overall\n\nTEST CASES TO ADD:\n- test_tokenize_snake_case()\n- test_tokenize_camel_case()\n- test_tokenize_with_special_chars()\n- test_keyword_search_finds_exact_function_names()\n\nLocation: src/mcpindexer/keyword_search.py","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-28T15:13:18.973099-05:00","updated_at":"2025-10-28T15:35:21.373784-05:00","closed_at":"2025-10-28T15:35:21.373784-05:00","dependencies":[{"issue_id":"mcpIndexer-130","depends_on_id":"mcpIndexer-128","type":"discovered-from","created_at":"2025-10-28T15:13:18.97387-05:00","created_by":"gkatechis"}]}
{"id":"mcpIndexer-131","title":"Phase 1: Test GPU acceleration for embedding generation","description":"**Goal**: Test GPU acceleration for embedding generation to achieve 20-24x speedup.\n\n**Expected Speedup**: 20-24x (based on sentence-transformers benchmarks)\n- CPU: 750 queries/sec\n- GPU: 18,000 queries/sec\n\n**Evidence**:\n- sentence-transformers documentation shows consistent 20-24x speedup across model types\n- MiniLM-L6 models: 24x speedup\n- DistilBERT models: 20x speedup\n- MPNet models: 23.5x speedup\n\n**Hardware Options**:\n1. NVIDIA GPU with CUDA (compute capability 7.0+)\n2. Apple Silicon Mac with MPS (Metal Performance Shaders)\n   - Requires macOS 12.3+, Python 3.7+\n   - Beta but production-ready\n\n**Implementation**:\n```python\n# Current\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Test\nmodel = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')\n# or for Mac\nmodel = SentenceTransformer('all-MiniLM-L6-v2', device='mps')\n```\n\n**Benchmark Test**:\n1. Run baseline: Current CPU performance on small + large repo\n2. Run GPU test: Same repos with device='cuda' or device='mps'\n3. Compare: Total indexing time, throughput (files/sec)\n\n**Success Criteria**: \n- \u003e10x overall speedup\n- Large repo (18 min) → \u003c2 minutes\n\n**If Successful**: This alone could solve the performance problem","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-28T15:26:22.3041-05:00","updated_at":"2025-10-28T15:29:50.088448-05:00","dependencies":[{"issue_id":"mcpIndexer-131","depends_on_id":"mcpIndexer-129","type":"discovered-from","created_at":"2025-10-28T15:26:22.305529-05:00","created_by":"gkatechis"}]}
{"id":"mcpIndexer-132","title":"Phase 2: Test sentence-transformers multi-process pool","description":"**Goal**: Test sentence-transformers' built-in multi-process pool for parallel embedding generation.\n\n**Expected Speedup**: 1.5-2x per additional device\n- With 4 CPUs: 1.5-2x\n- With 2 GPUs: 2-4x\n\n**Key Insight**: \nUnlike our failed parallel parsing attempt, this parallelizes EMBEDDING (70% of work, the actual bottleneck).\n\n**Why This Should Work**:\n- sentence-transformers designed this specifically for parallel embedding\n- Handles pickling, queue management, batching correctly\n- Distributes chunks across multiple processes\n- Each process runs on separate device (GPU or CPU)\n\n**Implementation**:\n```python\n# Start pool with available devices\npool = model.start_multi_process_pool(['cuda:0', 'cuda:1'])\n# or for multiple CPUs\npool = model.start_multi_process_pool(['cpu']*4)\n\n# Encode with pool\nembeddings = model.encode(sentences, pool=pool)\n\n# Clean up\nmodel.stop_multi_process_pool(pool)\n```\n\n**Benchmark Tests**:\n1. Baseline: Single device (CPU or GPU)\n2. Test: Multiple CPUs (if no GPU)\n3. Test: Multiple GPUs (if available)\n\n**Configuration to Test**:\n- 1 CPU (baseline)\n- 2 CPUs\n- 4 CPUs\n- 8 CPUs (if available)\n- 1 GPU + 1 GPU (if multiple GPUs available)\n\n**Success Criteria**:\n- \u003e1.5x speedup with 4 CPUs\n- \u003e2x speedup with 2 GPUs\n\n**Note**: Should test AFTER Phase 1 (GPU) to see if combining GPU + multi-process provides additional gains","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-28T15:26:28.821735-05:00","updated_at":"2025-10-28T15:29:56.66274-05:00","dependencies":[{"issue_id":"mcpIndexer-132","depends_on_id":"mcpIndexer-129","type":"discovered-from","created_at":"2025-10-28T15:26:28.823165-05:00","created_by":"gkatechis"}]}
{"id":"mcpIndexer-133","title":"Phase 3a: Test batch size optimization","description":"**Goal**: Find optimal batch size for embedding generation.\n\n**Expected Speedup**: 1.2-2x\n\n**Current**: Using default batch_size=32 in sentence-transformers\n\n**Theory**:\n- Larger batches = better GPU utilization\n- Too large = Out of Memory (OOM) errors\n- Optimal depends on model size and GPU memory\n\n**Implementation**:\n```python\n# Test different batch sizes\nembeddings = model.encode(texts, batch_size=batch_size)\n```\n\n**Benchmark Tests**:\nTest batch sizes: 16, 32 (baseline), 64, 128, 256, 512, 1024\n- Measure: Time per batch, throughput (chunks/sec)\n- Watch for: OOM errors, memory usage\n- Compare: CPU vs GPU (optimal may differ)\n\n**Expected Results**:\n- Small batches (16-32): Underutilized GPU\n- Medium batches (64-256): Optimal performance\n- Large batches (512+): Diminishing returns or OOM\n\n**Success Criteria**:\n- Find batch size with \u003e20% speedup over default\n- No OOM errors\n- Stable across different repo sizes\n\n**Implementation Complexity**: Trivial (one parameter)\n\n**Can Combine With**: GPU acceleration, multi-process pool","status":"open","priority":2,"issue_type":"task","created_at":"2025-10-28T15:26:35.275551-05:00","updated_at":"2025-10-28T15:30:03.251332-05:00","dependencies":[{"issue_id":"mcpIndexer-133","depends_on_id":"mcpIndexer-129","type":"discovered-from","created_at":"2025-10-28T15:26:35.276875-05:00","created_by":"gkatechis"}]}
{"id":"mcpIndexer-134","title":"Phase 3b: Test quantization (int8, binary embeddings)","description":"**Goal**: Test embedding quantization for speed/quality tradeoff.\n\n**Expected Speedup**: 2-3x (inference + storage)\n\n**Current**: Using float32 (full precision, 384 dimensions × 4 bytes = 1536 bytes per embedding)\n\n**Options**:\n1. **int8**: ~4x storage reduction, 2-3x faster\n   - 384 dimensions × 1 byte = 384 bytes\n   - Maintains reasonable quality\n   \n2. **uint8**: Similar to int8, unsigned values\n   \n3. **binary**: ~32x storage reduction, significant speedup\n   - 384 dimensions → 48 bytes (1 bit per dimension)\n   - More quality degradation\n\n**Implementation**:\n```python\n# Test different precisions\nembeddings = model.encode(texts, precision='int8')\nembeddings = model.encode(texts, precision='binary')\n```\n\n**Benchmark Tests**:\nFor each precision level:\n1. Speed: Time to generate embeddings\n2. Storage: Embedding size in memory/disk\n3. **Quality**: Semantic search accuracy\n   - Compare search results vs float32 baseline\n   - Use sample queries on indexed repo\n   - Measure: Top-k accuracy, ranking correlation\n\n**Quality Test Critical**:\nMust verify that quantization doesn't break semantic search for code.\n- Test on known code search queries\n- Compare top 10 results vs float32\n- Acceptable: \u003e90% overlap in top results\n\n**Success Criteria**:\n- 2x+ speedup\n- \u003c5% quality degradation\n- All tests pass with quantized embeddings\n\n**Risk**: May affect search quality for technical code terms\n**Fallback**: If quality drops, stay with float32\n\n**Can Combine With**: GPU acceleration, optimal batch size","status":"open","priority":2,"issue_type":"task","created_at":"2025-10-28T15:26:41.791348-05:00","updated_at":"2025-10-28T15:30:09.968753-05:00","dependencies":[{"issue_id":"mcpIndexer-134","depends_on_id":"mcpIndexer-129","type":"discovered-from","created_at":"2025-10-28T15:26:41.792946-05:00","created_by":"gkatechis"}]}
{"id":"mcpIndexer-135","title":"Create comprehensive performance benchmark script","description":"**Goal**: Create unified benchmark script to test all optimization strategies systematically.\n\n**Requirements**:\n1. Test multiple configurations in one run\n2. Output comparison table\n3. Measure both speed and quality\n4. Handle errors gracefully\n5. Support different hardware configurations\n\n**Configurations to Test**:\n```\n1. Baseline: CPU, batch_size=32, float32\n2. GPU: device='cuda/mps', batch_size=32, float32\n3. Batch sizes: [32, 64, 128, 256, 512] on GPU\n4. Quantization: ['float32', 'int8', 'binary'] on GPU\n5. Multi-process: [1, 2, 4, 8] processes\n```\n\n**Metrics to Capture**:\n- Total indexing time\n- Files per second\n- Chunks per second\n- Memory usage (peak)\n- Storage size (for quantization tests)\n- Quality metrics (for quantization)\n\n**Output Format**:\n```\nConfiguration               | Time    | Files/sec | Speedup | Quality\n-----------------------------------------------------------------\nCPU baseline (32)           | 1078s   | 10.4      | 1.0x    | 100%\nGPU (32)                    | 45s     | 249.5     | 24x     | 100%\nGPU batch=128               | 30s     | 374.3     | 36x     | 100%\nGPU batch=128 int8          | 15s     | 748.6     | 72x     | 98%\nGPU batch=128 binary        | 8s      | 1403.6    | 135x    | 85%\nMulti-process 4 CPUs        | 540s    | 20.8      | 2x      | 100%\n```\n\n**Script Features**:\n- Auto-detect available hardware (CUDA, MPS, CPU count)\n- Skip tests if hardware unavailable\n- Save results to JSON for comparison\n- Progress reporting\n- Error handling and cleanup\n\n**Test Repositories**:\n- Small: mcpIndexer (22 files, fast iteration)\n- Large: zendesk/zendesk (11k files, realistic workload)\n\n**Success Criteria**:\n- Runs all applicable tests automatically\n- Clear comparison of results\n- Identifies best configuration for hardware\n- Reproducible results","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-28T15:26:48.392015-05:00","updated_at":"2025-10-28T15:30:16.619219-05:00","dependencies":[{"issue_id":"mcpIndexer-135","depends_on_id":"mcpIndexer-129","type":"discovered-from","created_at":"2025-10-28T15:26:48.39354-05:00","created_by":"gkatechis"}]}
