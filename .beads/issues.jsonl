{"id":"mcpIndexer-128","title":"Add hybrid search combining semantic and exact match","description":"Implement hybrid search that combines semantic vector search with exact match (grep/ripgrep) capabilities.\n\nKEY APPROACHES FROM RESEARCH:\n1. Reciprocal Rank Fusion (RRF) - Most recommended method\n   - Formula: score = Σ 1/(k + rank(d)) where k=60 typically\n   - Combines ranked lists without needing score normalization\n   - Used by Elasticsearch, Weaviate, Qdrant\n\n2. Alpha Weighting\n   - alpha=1.0: pure semantic search\n   - alpha=0.5: balanced hybrid\n   - alpha=0.0: pure keyword search\n   - Scale scores: sparse*(1-alpha) + dense*alpha\n\n3. Query Fusion Pattern (from LlamaIndex)\n   - Run both retrievers in parallel\n   - Merge results using fusion algorithm\n   - Return top-k combined results\n\nCURRENT IMPLEMENTATION CONTEXT:\n=====================================\nArchitecture:\n- EmbeddingStore (embeddings.py): Manages ChromaDB + sentence-transformers\n  - semantic_search(): Vector search using dense embeddings\n  - find_by_symbol(): Exact symbol name lookup via ChromaDB metadata\n  - find_related_by_file(): Vector similarity for related code\n\n- MCP Server (server.py): Exposes tools to Claude Code\n  - semantic_search tool: Natural language code search\n  - find_definition tool: Uses find_by_symbol() for exact lookup\n  - find_references tool: Semantic search for symbol usages\n  - answer_question tool: Context retrieval for questions\n\n- Current Flow:\n  Index: Parse → Chunk (tree-sitter) → Embed (sentence-transformers) → Store (ChromaDB)\n  Search: Query → Embed → ChromaDB vector search → Results\n\nINTEGRATION PLAN:\n=================\n1. Add BM25/Keyword Search Layer\n   - New module: keyword_search.py\n   - Use rank-bm25 library or build simple TF-IDF\n   - Index code_text alongside embeddings\n   - Separate keyword index (or reuse ChromaDB documents)\n\n2. Implement Hybrid Search in EmbeddingStore\n   - New method: hybrid_search(query, alpha=0.5, n_results=10)\n   - Run semantic_search() and keyword_search() in parallel\n   - Combine using RRF (new utility: _reciprocal_rank_fusion())\n   - Return merged SearchResult list\n\n3. Update MCP Tools (server.py)\n   - Add 'search_mode' parameter to semantic_search tool\n     - Options: 'semantic', 'keyword', 'hybrid' (default)\n   - Add 'alpha' parameter for hybrid mode (default 0.5)\n   - Backward compatible: defaults to hybrid for best results\n\n4. Implementation Details\n   - RRF constant k=60 (industry standard)\n   - Keyword search: BM25 on code_text field\n   - Parallel execution for speed\n   - Return unified SearchResult objects with combined scores\n\n5. Testing Strategy\n   - Test exact string matches (keyword should shine)\n   - Test conceptual queries (semantic should shine)\n   - Test hybrid on mixed queries\n   - Verify RRF scoring makes sense\n\nBENEFITS:\n=========\n- Exact matches won't be missed (keyword search catches them)\n- Conceptual searches still work (semantic search)\n- Best of both worlds with RRF fusion\n- User can control with alpha parameter\n- Backward compatible with existing tools\n\nWork on separate feature branch: feature/hybrid-search","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-10-28T13:40:59.378578-05:00","updated_at":"2025-10-28T15:08:36.360174-05:00","closed_at":"2025-10-28T15:08:36.360174-05:00"}
{"id":"mcpIndexer-129","title":"Research: Optimize indexing performance - analyze bottlenecks and solutions","description":"**Goal**: Research and identify viable strategies to improve indexing performance.\n\n**Background**: \nAttempted parallel file processing (issue #106) but benchmarking revealed:\n- Parsing/chunking is only ~20% of indexing time\n- Embedding generation is ~70% of indexing time (the bottleneck)\n- Parallel processing of parsing actually made things 52-72% SLOWER due to multiprocessing overhead\n\n**Current Performance**:\n- Small repo (22 files): ~7.4 seconds (sequential)\n- Large repo (11,229 files): ~1,078 seconds (~18 minutes)\n- Throughput: 10-11 files/sec\n\n**Research Areas**:\n1. **GPU Acceleration** (issue #126): sentence-transformers supports CUDA\n   - Potential 10-50x speedup for embedding generation\n   - Need to investigate hardware requirements and setup\n\n2. **Batch Embedding Optimization**:\n   - Current batch size: 1000 chunks\n   - Investigate optimal batch sizes for different repo sizes\n   - Parallel batch processing strategies\n\n3. **Embedding Model Selection**:\n   - Current: all-MiniLM-L6-v2 (384 dimensions)\n   - Investigate faster models vs quality tradeoffs\n   - Quantization options\n\n4. **ChromaDB Configuration**:\n   - Investigate persistence settings\n   - Batch insert optimization\n   - Index configuration\n\n5. **Incremental Indexing Improvements**:\n   - Current: Re-embeds changed files\n   - Could we cache embeddings more effectively?\n\n**Deliverable**: \nDocument with performance analysis, recommendations, and estimated impact of each optimization strategy.","status":"open","priority":2,"issue_type":"task","created_at":"2025-10-28T14:59:09.979433-05:00","updated_at":"2025-10-28T14:59:35.099434-05:00"}
