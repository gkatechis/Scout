{"id":"mcpIndexer-126","title":"Add GPU acceleration support for embeddings","description":"**Goal**: Enable GPU acceleration for embedding generation to dramatically improve indexing speed.\n\n**What we're accomplishing**: Sentence-transformers supports CUDA/GPU acceleration which provides 10-50x speedup:\n- CPU: 170-750 queries/sec\n- GPU: 4,000-18,000 queries/sec\n\n**Impact**: Indexing large repositories becomes 10-50x faster. A repo that takes 10 minutes could index in 12-60 seconds.\n\n**Implementation**:\n- Auto-detect GPU availability (torch.cuda.is_available())\n- Add device parameter to SentenceTransformer initialization\n- Add configuration option to force CPU/GPU\n- Update documentation with GPU setup instructions\n- Gracefully fallback to CPU if GPU unavailable","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-10-27T20:33:48.924899-05:00","updated_at":"2025-10-29T09:14:28.403303-05:00","closed_at":"2025-10-29T09:14:28.403303-05:00"}
{"id":"mcpIndexer-131","title":"Phase 1: Test GPU acceleration for embedding generation","description":"**Goal**: Test GPU acceleration for embedding generation to achieve 20-24x speedup.\n\n**Expected Speedup**: 20-24x (based on sentence-transformers benchmarks)\n- CPU: 750 queries/sec\n- GPU: 18,000 queries/sec\n\n**Evidence**:\n- sentence-transformers documentation shows consistent 20-24x speedup across model types\n- MiniLM-L6 models: 24x speedup\n- DistilBERT models: 20x speedup\n- MPNet models: 23.5x speedup\n\n**Hardware Options**:\n1. NVIDIA GPU with CUDA (compute capability 7.0+)\n2. Apple Silicon Mac with MPS (Metal Performance Shaders)\n   - Requires macOS 12.3+, Python 3.7+\n   - Beta but production-ready\n\n**Implementation**:\n```python\n# Current\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Test\nmodel = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')\n# or for Mac\nmodel = SentenceTransformer('all-MiniLM-L6-v2', device='mps')\n```\n\n**Benchmark Test**:\n1. Run baseline: Current CPU performance on small + large repo\n2. Run GPU test: Same repos with device='cuda' or device='mps'\n3. Compare: Total indexing time, throughput (files/sec)\n\n**Success Criteria**: \n- \u003e10x overall speedup\n- Large repo (18 min) → \u003c2 minutes\n\n**If Successful**: This alone could solve the performance problem","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-28T15:26:22.3041-05:00","updated_at":"2025-10-29T09:14:50.733613-05:00","closed_at":"2025-10-29T09:14:50.733613-05:00","dependencies":[{"issue_id":"mcpIndexer-131","depends_on_id":"mcpIndexer-129","type":"discovered-from","created_at":"2025-10-28T15:26:22.305529-05:00","created_by":"gkatechis"}]}
{"id":"mcpIndexer-132","title":"Phase 2: Test sentence-transformers multi-process pool","description":"**Goal**: Test sentence-transformers' built-in multi-process pool for parallel embedding generation.\n\n**Expected Speedup**: 1.5-2x per additional device\n- With 4 CPUs: 1.5-2x\n- With 2 GPUs: 2-4x\n\n**Key Insight**: \nUnlike our failed parallel parsing attempt, this parallelizes EMBEDDING (70% of work, the actual bottleneck).\n\n**Why This Should Work**:\n- sentence-transformers designed this specifically for parallel embedding\n- Handles pickling, queue management, batching correctly\n- Distributes chunks across multiple processes\n- Each process runs on separate device (GPU or CPU)\n\n**Implementation**:\n```python\n# Start pool with available devices\npool = model.start_multi_process_pool(['cuda:0', 'cuda:1'])\n# or for multiple CPUs\npool = model.start_multi_process_pool(['cpu']*4)\n\n# Encode with pool\nembeddings = model.encode(sentences, pool=pool)\n\n# Clean up\nmodel.stop_multi_process_pool(pool)\n```\n\n**Benchmark Tests**:\n1. Baseline: Single device (CPU or GPU)\n2. Test: Multiple CPUs (if no GPU)\n3. Test: Multiple GPUs (if available)\n\n**Configuration to Test**:\n- 1 CPU (baseline)\n- 2 CPUs\n- 4 CPUs\n- 8 CPUs (if available)\n- 1 GPU + 1 GPU (if multiple GPUs available)\n\n**Success Criteria**:\n- \u003e1.5x speedup with 4 CPUs\n- \u003e2x speedup with 2 GPUs\n\n**Note**: Should test AFTER Phase 1 (GPU) to see if combining GPU + multi-process provides additional gains","notes":"Lower priority - multi-process can be tested separately if needed. GPU already working.","status":"open","priority":3,"issue_type":"task","created_at":"2025-10-28T15:26:28.821735-05:00","updated_at":"2025-10-29T09:14:56.381501-05:00","dependencies":[{"issue_id":"mcpIndexer-132","depends_on_id":"mcpIndexer-129","type":"discovered-from","created_at":"2025-10-28T15:26:28.823165-05:00","created_by":"gkatechis"}]}
{"id":"mcpIndexer-135","title":"Create comprehensive performance benchmark script","description":"**Goal**: Create unified benchmark script to test all optimization strategies systematically.\n\n**Requirements**:\n1. Test multiple configurations in one run\n2. Output comparison table\n3. Measure both speed and quality\n4. Handle errors gracefully\n5. Support different hardware configurations\n\n**Configurations to Test**:\n```\n1. Baseline: CPU, batch_size=32, float32\n2. GPU: device='cuda/mps', batch_size=32, float32\n3. Batch sizes: [32, 64, 128, 256, 512] on GPU\n4. Quantization: ['float32', 'int8', 'binary'] on GPU\n5. Multi-process: [1, 2, 4, 8] processes\n```\n\n**Metrics to Capture**:\n- Total indexing time\n- Files per second\n- Chunks per second\n- Memory usage (peak)\n- Storage size (for quantization tests)\n- Quality metrics (for quantization)\n\n**Output Format**:\n```\nConfiguration               | Time    | Files/sec | Speedup | Quality\n-----------------------------------------------------------------\nCPU baseline (32)           | 1078s   | 10.4      | 1.0x    | 100%\nGPU (32)                    | 45s     | 249.5     | 24x     | 100%\nGPU batch=128               | 30s     | 374.3     | 36x     | 100%\nGPU batch=128 int8          | 15s     | 748.6     | 72x     | 98%\nGPU batch=128 binary        | 8s      | 1403.6    | 135x    | 85%\nMulti-process 4 CPUs        | 540s    | 20.8      | 2x      | 100%\n```\n\n**Script Features**:\n- Auto-detect available hardware (CUDA, MPS, CPU count)\n- Skip tests if hardware unavailable\n- Save results to JSON for comparison\n- Progress reporting\n- Error handling and cleanup\n\n**Test Repositories**:\n- Small: mcpIndexer (22 files, fast iteration)\n- Large: zendesk/zendesk (11k files, realistic workload)\n\n**Success Criteria**:\n- Runs all applicable tests automatically\n- Clear comparison of results\n- Identifies best configuration for hardware\n- Reproducible results","notes":"Comprehensive benchmarking can be done later. Basic GPU benchmarks already exist.","status":"open","priority":3,"issue_type":"task","created_at":"2025-10-28T15:26:48.392015-05:00","updated_at":"2025-10-29T09:15:02.212826-05:00","dependencies":[{"issue_id":"mcpIndexer-135","depends_on_id":"mcpIndexer-129","type":"discovered-from","created_at":"2025-10-28T15:26:48.39354-05:00","created_by":"gkatechis"}]}
{"id":"mcpIndexer-136","title":"Rename mcpIndexer to Scout","description":"","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-10-28T21:01:37.321767-05:00","updated_at":"2025-10-29T09:15:36.668885-05:00","closed_at":"2025-10-29T09:15:36.668885-05:00"}
{"id":"mcpIndexer-143","title":"Phase 7: GitHub repo rename and directory rename","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-28T21:02:25.945828-05:00","updated_at":"2025-10-29T09:15:31.181473-05:00","closed_at":"2025-10-29T09:15:31.181473-05:00","dependencies":[{"issue_id":"mcpIndexer-143","depends_on_id":"mcpIndexer-136","type":"discovered-from","created_at":"2025-10-28T21:02:25.947045-05:00","created_by":"gkatechis"}]}
{"id":"mcpIndexer-144","title":"Research spike: Fine-tuning embedding model for improved code search accuracy","description":"**Problem**: Current semantic search model (multi-qa-mpnet-base-dot-v1) has accuracy issues. Embedded comment claims '100% accuracy on code search' but this is demonstrably false. Need to explore options for improving search quality.\n\n**Research Questions**:\n1. Can fine-tuning the embedding model improve code search accuracy?\n2. What's the effort vs. benefit tradeoff for each approach?\n3. Which approach provides best ROI?\n\n**Options to Investigate**:\n\n**1. Fine-tuning (Highest potential accuracy gain)**\n- Collect failed search examples from Scout usage\n- Create training dataset: (query, relevant_code, irrelevant_code) triplets\n- Fine-tune using contrastive loss (sentence-transformers supports this)\n- Tools: sentence-transformers library has built-in training APIs\n- **Effort**: High (need dataset, GPU infrastructure, ongoing maintenance)\n- **Risk**: Requires 100+ quality examples, GPU training time, retraining on updates\n\n**2. Hybrid Search (Already implemented, needs testing)**\n- Feature exists in feature/hybrid-search branch (mcpIndexer-128)\n- Combines semantic + keyword/BM25 search\n- Compensates for semantic model weaknesses\n- **Effort**: Low (just testing and tuning)\n- **Quick win**: Should catch exact matches semantic search misses\n\n**3. Code-specific Models**\n- Try models trained on code: \n  - microsoft/codebert-base\n  - Salesforce/codet5-base\n  - microsoft/unixcoder-base\n- Current model is Q\u0026A-focused, not code-focused\n- **Effort**: Low (model swap, reindex, test)\n- **Issue**: mcpIndexer-108 covers this already\n\n**4. Prompt Engineering for Context**\n- Enhance context_text prepended to chunks\n- Add more file path, class hierarchy, docstring info\n- Already using context_text but could improve\n- **Effort**: Low (chunker.py changes)\n\n**Success Criteria**:\n- Collect 10-20 failed search examples\n- Test each approach against failures\n- Measure improvement in search accuracy\n- Document effort/benefit for each approach\n- Recommend approach(es) to implement\n\n**First Step**: \nCollect real failed search examples to create benchmark dataset\n\n**Timeline**: 1-2 weeks for research spike","status":"open","priority":2,"issue_type":"task","created_at":"2025-10-29T10:49:49.157324-05:00","updated_at":"2025-10-29T10:50:16.681764-05:00","dependencies":[{"issue_id":"mcpIndexer-144","depends_on_id":"mcpIndexer-145","type":"parent-child","created_at":"2025-10-29T11:17:51.723784-05:00","created_by":"gkatechis"}]}
{"id":"mcpIndexer-145","title":"Epic: Technology Stack Evaluation - Validate core architectural decisions","description":"**Goal**: Comprehensively evaluate whether Scout's core technology choices are optimal for code semantic search and indexing at scale.\n\n**Context**: Scout was built with specific technology choices (Python, ChromaDB, sentence-transformers, tree-sitter). Before scaling further or making major architectural commitments, we need evidence-based validation that these are the best choices for our use case.\n\n**Scope**: Deep research and benchmarking of:\n1. Programming language (Python vs alternatives)\n2. Vector database (ChromaDB vs alternatives)\n3. Code parsing library (tree-sitter vs alternatives)\n4. Embedding framework (sentence-transformers vs alternatives)\n\n**Research Requirements**:\n- Minimum 5, maximum 20 data sources per component\n- Real-world benchmarks, not just marketing claims\n- Cost analysis (development time, runtime, infrastructure)\n- Community/ecosystem evaluation\n- Migration effort estimation if switching\n\n**Deliverables**:\n- Research reports with cited sources\n- Benchmark results on Scout's actual workloads\n- Decision matrix with weighted criteria\n- Migration plans if changes recommended\n- Risk assessment for each alternative\n\n**Timeline**: 3-4 weeks for comprehensive evaluation\n\n**Sub-tasks**: See linked issues below","status":"open","priority":1,"issue_type":"epic","created_at":"2025-10-29T11:01:38.852574-05:00","updated_at":"2025-10-29T11:01:58.7152-05:00"}
{"id":"mcpIndexer-146","title":"Research: Python vs alternative languages for code indexing performance","description":"**Objective**: Determine if Python is the optimal language for Scout's code indexing and semantic search use case through comprehensive research and benchmarking.\n\n**Current State**: \n- Python chosen for ML ecosystem (sentence-transformers, PyTorch)\n- Good for prototyping\n- Strong tree-sitter bindings\n- Large community\n\n**Research Question**: \nIs Python the best choice for production-scale code indexing, or would an alternative language provide better performance, maintainability, or operational characteristics?\n\n**Alternatives to Evaluate**:\n\n1. **Rust**\n   - Strengths: Memory safety, performance, growing ML ecosystem (candle, burn.rs)\n   - Concerns: Development velocity, smaller ML ecosystem\n   - Vector DB clients: qdrant-client, similar available\n\n2. **Go**\n   - Strengths: Concurrency, simple deployment, fast compilation\n   - Concerns: Weaker ML ecosystem, no official sentence-transformers\n   - Vector DB clients: Native support in most DBs\n\n3. **TypeScript/Node.js**\n   - Strengths: JavaScript ecosystem, V8 JIT, async I/O\n   - Concerns: ML ecosystem less mature than Python\n   - Vector DB clients: Good support\n\n4. **Java/Kotlin**\n   - Strengths: JVM maturity, DJL (Deep Java Library), enterprise support\n   - Concerns: Heavier runtime, more verbose\n   - Vector DB clients: Excellent support\n\n5. **C++**\n   - Strengths: Maximum performance, direct hardware control\n   - Concerns: Development complexity, memory management\n   - Vector DB clients: Available but varies\n\n**Evaluation Criteria** (weighted):\n\n1. **Performance (30%)**\n   - Indexing throughput (files/sec)\n   - Query latency (p50, p95, p99)\n   - Memory efficiency\n   - CPU utilization\n   - Cold start time\n   - Concurrent request handling\n\n2. **ML/AI Ecosystem (25%)**\n   - Sentence transformer availability\n   - PyTorch/ONNX support\n   - Embedding model ecosystem\n   - GPU acceleration support\n   - Model serving libraries\n\n3. **Code Parsing Ecosystem (15%)**\n   - Tree-sitter binding quality\n   - Language grammar availability\n   - Parsing performance\n   - AST manipulation capabilities\n\n4. **Development Velocity (10%)**\n   - Time to implement prototype\n   - Code maintainability\n   - Debugging tools\n   - Testing frameworks\n   - Package management\n\n5. **Operational Characteristics (10%)**\n   - Deployment complexity\n   - Binary/image size\n   - Resource requirements\n   - Monitoring/observability\n   - Error handling\n\n6. **Ecosystem Maturity (10%)**\n   - Vector DB client libraries\n   - MCP integration options\n   - Community size\n   - Package availability\n   - Documentation quality\n\n**Data Sources Required (5-20 per language)**:\n\n**Performance Evidence**:\n- [ ] Official language benchmarks (benchmarksgame, etc.)\n- [ ] ML inference benchmarks (ONNX, TensorFlow Lite, etc.)\n- [ ] Vector search implementations in each language\n- [ ] Real-world production case studies\n- [ ] Academic papers comparing language performance for ML workloads\n- [ ] Memory profiling studies\n- [ ] Concurrency benchmarks\n- [ ] Startup time comparisons\n\n**Ecosystem Evidence**:\n- [ ] Package registry metrics (PyPI, crates.io, npm, Maven Central)\n- [ ] GitHub statistics (projects, stars, contributors)\n- [ ] Stack Overflow question counts\n- [ ] Job market data (hiring trends)\n- [ ] Survey data (Stack Overflow, JetBrains developer surveys)\n- [ ] ML framework availability matrix\n- [ ] Vector DB client feature comparison\n\n**Production Evidence**:\n- [ ] Case studies of similar projects in each language\n- [ ] Semantic search implementations analysis\n- [ ] Code indexing tools comparison (Sourcegraph, GitHub Copilot backends)\n- [ ] Migration stories (Python → X or X → Python)\n- [ ] Cost analysis (AWS/GCP compute costs per language)\n- [ ] Maintenance burden reports\n- [ ] Team productivity studies\n\n**Specific Projects to Analyze**:\n- [ ] Sourcegraph (Go) - code search at scale\n- [ ] Meilisearch (Rust) - search engine\n- [ ] Vespa (Java) - vector search engine\n- [ ] Qdrant (Rust) - vector database\n- [ ] Weaviate (Go) - vector database\n- [ ] TypeSense (C++) - search engine\n- [ ] Tantivy (Rust) - full-text search\n\n**Benchmarks to Conduct**:\n\n1. **Index Small Repo (Scout codebase)**\n   - Implement minimal indexer in each language\n   - Measure time, memory, CPU\n   - Compare code complexity\n\n2. **Index Large Repo (zendesk/zendesk)**\n   - Same metrics at scale\n   - Test concurrency benefits\n   - Measure resource utilization\n\n3. **Query Performance**\n   - Latency under load\n   - Concurrent query handling\n   - Memory stability\n\n4. **Operational Metrics**\n   - Docker image size\n   - Cold start time\n   - Crash recovery\n   - Resource limits behavior\n\n**Deliverables**:\n\n1. **Research Report** (Markdown document)\n   - Executive summary with recommendation\n   - Detailed analysis for each language\n   - Cited sources (minimum 5, max 20 per language)\n   - Decision matrix with weighted scores\n\n2. **Benchmark Results**\n   - Raw data (CSV/JSON)\n   - Visualization (charts/graphs)\n   - Statistical analysis\n   - Reproducible benchmark scripts\n\n3. **Cost Analysis**\n   - Development time estimates\n   - Infrastructure cost projections\n   - Migration effort if switching\n   - Maintenance overhead\n\n4. **Risk Assessment**\n   - Technical risks per alternative\n   - Mitigation strategies\n   - Unknown unknowns identified\n\n5. **Recommendation**\n   - Stay with Python OR switch to X\n   - Evidence-based justification\n   - Migration plan if switching\n   - Timeline and resource requirements\n\n**Timeline**: 1.5-2 weeks\n\n**Success Criteria**:\n- Minimum 25 total data sources gathered (5 per language × 5 languages)\n- All benchmarks executed on consistent hardware\n- Decision matrix completed with objective scoring\n- Clear recommendation with evidence\n- All team members can review and understand findings\n\n**Open Questions**:\n- Can we leverage Python for prototyping and optimize hot paths in Rust/C++?\n- Is polyglot approach worth the complexity?\n- What's the migration cost if we switch?\n- How does language choice affect hiring?\n\n**Next Steps After Research**:\n- If Python optimal: Document why, close issue\n- If alternative better: Create migration plan, estimate effort\n- If inconclusive: Identify what additional data needed","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-29T11:02:09.34628-05:00","updated_at":"2025-10-29T11:03:28.883795-05:00","dependencies":[{"issue_id":"mcpIndexer-146","depends_on_id":"mcpIndexer-145","type":"parent-child","created_at":"2025-10-29T11:02:34.426568-05:00","created_by":"gkatechis"}]}
{"id":"mcpIndexer-147","title":"Research: ChromaDB vs alternative vector databases for code search","description":"**Objective**: Determine if ChromaDB is the optimal vector database for Scout's code search use case through comprehensive research and benchmarking.\n\n**Current State**:\n- ChromaDB chosen for embedded/local-first approach\n- Python-native, easy integration\n- Sufficient for prototyping\n- No server management required\n\n**Research Question**:\nIs ChromaDB the best choice for production-scale code search, or would an alternative vector database provide better performance, scalability, features, or operational characteristics?\n\n**Alternatives to Evaluate**:\n\n1. **Qdrant**\n   - Strengths: Rust performance, excellent filtering, hybrid search, cloud + self-hosted\n   - Deployment: Server required, Docker-friendly\n   - Filtering: Advanced payload filtering\n   - Scaling: Horizontal scaling, sharding\n\n2. **Weaviate**\n   - Strengths: Go performance, GraphQL API, schema support, vectorizer plugins\n   - Deployment: Server required, Kubernetes-native\n   - Filtering: Schema-based filtering, cross-references\n   - Scaling: Multi-tenancy, replication\n\n3. **Pinecone**\n   - Strengths: Fully managed, serverless, auto-scaling\n   - Deployment: Cloud-only (SaaS)\n   - Filtering: Metadata filtering\n   - Scaling: Automatic, handles billions of vectors\n\n4. **Milvus**\n   - Strengths: Production-grade, massive scale, CNCF project\n   - Deployment: Complex (Kubernetes, multiple components)\n   - Filtering: Attribute filtering\n   - Scaling: Distributed, handles trillions of vectors\n\n5. **pgvector (PostgreSQL)**\n   - Strengths: Use existing PostgreSQL, ACID guarantees, mature ecosystem\n   - Deployment: PostgreSQL extension\n   - Filtering: Full SQL capabilities\n   - Scaling: PostgreSQL scaling strategies\n\n6. **FAISS (Facebook AI Similarity Search)**\n   - Strengths: Extremely fast, research-backed, GPU support\n   - Deployment: Library (not a database)\n   - Filtering: Limited, requires custom code\n   - Scaling: In-memory, single machine focus\n\n7. **Elasticsearch/OpenSearch**\n   - Strengths: Full-text + vector, mature ecosystem, battle-tested\n   - Deployment: Cluster recommended\n   - Filtering: Extensive query DSL\n   - Scaling: Proven at massive scale\n\n8. **Redis with Vector Search**\n   - Strengths: In-memory speed, existing Redis expertise\n   - Deployment: Redis server + module\n   - Filtering: Redis data structures\n   - Scaling: Redis scaling patterns\n\n9. **LanceDB**\n   - Strengths: Disk-based, serverless, versioning, SQL\n   - Deployment: Embedded like ChromaDB\n   - Filtering: SQL filtering\n   - Scaling: Cloud object storage\n\n10. **Vespa**\n    - Strengths: Production-grade, real-time, hybrid search, Yahoo-backed\n    - Deployment: Complex (multiple services)\n    - Filtering: Advanced query language\n    - Scaling: Distributed, handles billions\n\n**Evaluation Criteria** (weighted):\n\n1. **Query Performance (25%)**\n   - Vector search latency (p50, p95, p99)\n   - Throughput (queries/sec)\n   - Concurrent query handling\n   - Filter performance\n   - Hybrid search speed\n   - ANN accuracy vs speed tradeoff\n\n2. **Indexing Performance (20%)**\n   - Insert throughput (vectors/sec)\n   - Batch insert performance\n   - Index build time\n   - Update performance\n   - Delete performance\n\n3. **Scalability (15%)**\n   - Maximum vectors supported\n   - Horizontal scaling capabilities\n   - Sharding support\n   - Multi-tenancy\n   - Resource requirements at scale\n\n4. **Features (15%)**\n   - Metadata filtering capabilities\n   - Hybrid search (vector + keyword)\n   - Multiple index types (HNSW, IVF, etc.)\n   - Backup/restore\n   - Versioning/snapshots\n   - Multi-vector support\n   - Distance metrics supported\n\n5. **Operational Characteristics (15%)**\n   - Deployment complexity\n   - Memory usage\n   - Disk usage/efficiency\n   - Reliability/uptime\n   - Monitoring/observability\n   - Upgrade path\n   - Self-hosted vs managed options\n\n6. **Developer Experience (10%)**\n   - API quality\n   - Python client quality\n   - Documentation\n   - Community support\n   - Error messages\n   - Debugging tools\n   - Local development story\n\n**Data Sources Required (5-20 per database)**:\n\n**Performance Evidence**:\n- [ ] ann-benchmarks.com comparisons (industry standard)\n- [ ] Official benchmark results from vendors\n- [ ] Third-party independent benchmarks\n- [ ] Academic papers on vector search algorithms\n- [ ] Performance at scale case studies\n- [ ] Latency under load tests\n- [ ] Memory usage studies\n- [ ] Concurrent query benchmarks\n\n**Production Evidence**:\n- [ ] Case studies from production deployments\n- [ ] Scale stories (vectors handled, QPS achieved)\n- [ ] Reliability reports (uptime, failure modes)\n- [ ] Cost analyses (infrastructure costs)\n- [ ] Migration stories (X → Y database)\n- [ ] Operational complexity assessments\n- [ ] Real-world performance reports\n- [ ] Incident post-mortems\n\n**Ecosystem Evidence**:\n- [ ] GitHub activity (commits, issues, PRs)\n- [ ] Community size (Discord, Slack, forums)\n- [ ] Client library quality comparison\n- [ ] Integration ecosystem (Langchain, LlamaIndex, etc.)\n- [ ] Cloud provider support\n- [ ] Managed service options\n- [ ] Enterprise adoption metrics\n\n**Feature Evidence**:\n- [ ] Feature comparison matrices\n- [ ] Filtering capability benchmarks\n- [ ] Hybrid search quality tests\n- [ ] Backup/restore procedure documentation\n- [ ] Multi-tenancy architecture analysis\n- [ ] Security features comparison\n\n**Specific Deployments to Analyze**:\n- [ ] Notion (Qdrant for search)\n- [ ] Character.AI (Pinecone for character memory)\n- [ ] Reddit (Milvus for recommendations)\n- [ ] Stack Overflow (Elasticsearch for search)\n- [ ] Shopify (custom FAISS deployment)\n- [ ] OpenAI (Pinecone for embeddings API)\n- [ ] Sourcegraph (PostgreSQL + custom indexes)\n\n**Benchmarks to Conduct**:\n\n1. **Index Scout Codebase**\n   - Time to index all embeddings\n   - Memory usage during indexing\n   - Disk space required\n   - Implementation complexity\n\n2. **Index Large Repo (zendesk/zendesk - ~50k chunks)**\n   - Indexing time\n   - Resource utilization\n   - Query performance at scale\n   - Filter performance\n\n3. **Query Performance**\n   - Simple vector search (no filters)\n   - Filtered vector search (repo filter)\n   - Complex multi-filter queries\n   - Hybrid search (if supported)\n   - Concurrent queries (10, 50, 100)\n\n4. **Operational Tests**\n   - Backup/restore time\n   - Crash recovery\n   - Upgrade procedure\n   - Memory limits behavior\n   - Disk full handling\n\n5. **Feature Tests**\n   - Metadata filtering accuracy\n   - Distance metric comparison\n   - Update/delete operations\n   - Multi-collection management\n\n6. **Cost Analysis**\n   - Self-hosted compute costs (AWS/GCP)\n   - Managed service costs (Pinecone tier pricing)\n   - Storage costs\n   - Network costs\n\n**Special Considerations for Code Search**:\n\n1. **Multi-tenancy**: Can we isolate multiple users/orgs?\n2. **Filtering**: Critical for repo/language/file filters\n3. **Updates**: Code changes frequently, need fast updates\n4. **Hybrid search**: Combining semantic + keyword crucial\n5. **Local development**: Can developers run it locally?\n6. **Scale**: 10-100s of repos, 100k-10M+ code chunks\n7. **Metadata**: Need rich metadata (file, repo, language, symbols, etc.)\n\n**Deliverables**:\n\n1. **Research Report** (Markdown document)\n   - Executive summary with recommendation\n   - Detailed analysis for each database\n   - Cited sources (minimum 50 total, 5-10 per database)\n   - Decision matrix with weighted scores\n   - Feature comparison table\n\n2. **Benchmark Results**\n   - Raw data (CSV/JSON)\n   - Visualization (charts comparing all databases)\n   - Statistical analysis\n   - Reproducible benchmark scripts\n   - Docker Compose files for testing\n\n3. **Cost Analysis**\n   - Self-hosted infrastructure costs\n   - Managed service pricing comparison\n   - Migration effort estimates\n   - Operational overhead comparison\n\n4. **Risk Assessment**\n   - Technical risks per alternative\n   - Vendor lock-in concerns\n   - Community sustainability\n   - Breaking change history\n   - Mitigation strategies\n\n5. **Migration Plan** (if switching)\n   - Step-by-step migration procedure\n   - Data export/import strategy\n   - Downtime estimates\n   - Rollback plan\n   - Testing strategy\n\n6. **Recommendation**\n   - Stay with ChromaDB OR switch to X\n   - Evidence-based justification\n   - Timeline and resource requirements\n   - Phased rollout plan if switching\n\n**Timeline**: 2-3 weeks\n\n**Success Criteria**:\n- Minimum 50 total data sources (5-10 per database)\n- All benchmarks executed on identical hardware/data\n- ann-benchmarks.com data incorporated\n- Real production case studies reviewed\n- Decision matrix completed with objective scoring\n- Clear recommendation with supporting evidence\n- Migration plan drafted (if switching recommended)\n\n**Open Questions**:\n- Can we start with embedded (ChromaDB/LanceDB) and migrate to server (Qdrant/Weaviate) later?\n- What's the performance ceiling of embedded databases?\n- Is managed service worth the cost? (Pinecone ~$70/month vs self-hosted)\n- Do we need distributed vector DB for Scout's scale?\n- Should we optimize for single-user or multi-tenant future?\n\n**Critical Factors**:\n- ChromaDB may be sufficient for current scale\n- Performance matters less than correctness for code search\n- Hybrid search capability is critical (semantic + keyword)\n- Operational simplicity important for small team\n- Local development experience crucial\n\n**Next Steps After Research**:\n- If ChromaDB optimal: Document why, optimize current usage\n- If alternative better: Create detailed migration plan\n- If multiple good options: Define decision criteria for choosing\n- If inconclusive: Identify missing data, extend research","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-29T11:03:49.172321-05:00","updated_at":"2025-10-29T11:04:59.642326-05:00","dependencies":[{"issue_id":"mcpIndexer-147","depends_on_id":"mcpIndexer-145","type":"parent-child","created_at":"2025-10-29T11:04:00.081355-05:00","created_by":"gkatechis"}]}
{"id":"mcpIndexer-148","title":"Research: tree-sitter vs alternative code parsing libraries","description":"**Objective**: Determine if tree-sitter is the optimal code parsing library for Scout's multi-language code indexing through comprehensive research and benchmarking.\n\n**Current State**:\n- tree-sitter chosen for multi-language AST parsing\n- Fast, incremental parsing capabilities\n- Good error recovery\n- Used to extract functions, classes, symbols from code\n\n**Research Question**:\nIs tree-sitter the best choice for parsing code at scale, or would an alternative provide better performance, accuracy, language support, or features?\n\n**Alternatives to Evaluate**:\n\n1. **Language-Specific Native Parsers**\n   - Python: `ast` module, `parso`, `lib2to3`\n   - JavaScript/TypeScript: `@babel/parser`, `typescript` compiler API\n   - Go: `go/parser`, `go/ast`\n   - Rust: `syn`, `rustc` parser\n   - Java: `JavaParser`, Eclipse JDT\n   - Strengths: Most accurate, idiomatic, feature-rich\n   - Concerns: Requires separate parser per language, more maintenance\n\n2. **ANTLR (Another Tool for Language Recognition)**\n   - Strengths: Parser generator, many grammars available, LL(*) parsing\n   - Deployment: Grammar files + runtime\n   - Languages: 50+ grammars (Python, Java, C++, Go, etc.)\n   - Concerns: Grammar maintenance, performance overhead\n\n3. **srcML**\n   - Strengths: Converts source code to XML, preserves structure\n   - Languages: C, C++, C#, Java, Python\n   - Concerns: XML overhead, limited language support\n\n4. **Universal-ctags**\n   - Strengths: Fast tagging, lightweight, many languages\n   - Concerns: Symbol extraction only, no full AST\n   - Use case: Tags vs full parsing\n\n5. **Semgrep**\n   - Strengths: Pattern matching, security focus, good ASTs\n   - Languages: 30+ languages\n   - Concerns: Designed for pattern matching, not indexing\n\n6. **Sourcetrail**\n   - Strengths: Code indexing focus, relationship mapping\n   - Concerns: Full application, not library\n\n7. **Joern**\n   - Strengths: Code property graphs, security analysis\n   - Languages: C, C++, JVM languages, JavaScript\n   - Concerns: Complex setup, focused on security analysis\n\n8. **Language Server Protocol (LSP) Parsers**\n   - Use existing LSP servers (pyright, rust-analyzer, etc.)\n   - Strengths: Most accurate, IDE-grade parsing\n   - Concerns: Heavy, designed for interactive use\n\n9. **Polyglot (Microsoft)**\n   - Strengths: Multi-language code analysis framework\n   - Concerns: Research project, may not be production-ready\n\n10. **Regex + Heuristics**\n    - Strengths: Fast, simple, no dependencies\n    - Concerns: Fragile, inaccurate, limited to simple patterns\n\n**Evaluation Criteria** (weighted):\n\n1. **Parsing Speed (25%)**\n   - Time to parse Scout codebase\n   - Time to parse large repo (zendesk/zendesk)\n   - Incremental parsing support\n   - Memory usage during parsing\n   - CPU efficiency\n\n2. **Language Support (25%)**\n   - Number of languages supported\n   - Quality of language grammars\n   - Completeness of AST\n   - Error recovery quality\n   - Grammar maintenance/updates\n\n3. **AST Quality (20%)**\n   - Completeness (all syntax captured?)\n   - Accuracy (correct structure?)\n   - Ease of traversal\n   - Metadata richness\n   - Error node handling\n\n4. **Integration Ease (15%)**\n   - Python bindings quality\n   - API simplicity\n   - Documentation\n   - Installation complexity\n   - Dependencies\n\n5. **Maintenance (10%)**\n   - Community activity\n   - Grammar update frequency\n   - Breaking changes history\n   - Long-term viability\n\n6. **Features (5%)**\n   - Incremental parsing\n   - Syntax highlighting data\n   - Error recovery\n   - Multi-file context\n   - Semantic analysis\n\n**Data Sources Required (5-20 per parser)**:\n\n**Performance Evidence**:\n- [ ] Official benchmarks (parsing speed, memory)\n- [ ] Independent parsing benchmarks\n- [ ] Large codebase parsing reports\n- [ ] Academic papers on parsing algorithms\n- [ ] Performance profiling studies\n- [ ] Incremental parsing benchmarks\n\n**Language Support Evidence**:\n- [ ] Grammar repository analysis (completeness)\n- [ ] Language coverage comparison\n- [ ] Grammar quality assessments\n- [ ] Error recovery testing results\n- [ ] Real-world parsing accuracy tests\n\n**Production Evidence**:\n- [ ] Tools using each parser (GitHub code search, Sourcegraph, etc.)\n- [ ] Case studies from production deployments\n- [ ] Developer experience reports\n- [ ] Migration stories\n- [ ] Maintenance burden assessments\n\n**Ecosystem Evidence**:\n- [ ] GitHub activity (commits, issues, stars)\n- [ ] Language bindings quality\n- [ ] Integration examples\n- [ ] Community size\n- [ ] Corporate backing/support\n\n**Specific Tools Using Each Parser**:\n- [ ] tree-sitter: GitHub code nav, Atom, Neovim, Emacs\n- [ ] ANTLR: IntelliJ IDEA (partial), many academic tools\n- [ ] Language-specific: Most IDEs (PyCharm uses Python ast, etc.)\n- [ ] Semgrep: Semgrep CLI, security scanning\n- [ ] LSP: VS Code, many modern editors\n\n**Benchmarks to Conduct**:\n\n1. **Parse Scout Codebase (22 Python files)**\n   - Time to parse all files\n   - Memory usage\n   - AST completeness check\n   - Implementation complexity\n\n2. **Parse Large Repo (zendesk/zendesk - 11k files)**\n   - Time to parse (single-threaded)\n   - Time to parse (parallel)\n   - Memory usage at scale\n   - Error handling behavior\n\n3. **AST Quality Tests**\n   - Extract all functions/classes\n   - Extract imports correctly\n   - Handle syntax errors gracefully\n   - Capture docstrings/comments\n   - Preserve location info\n\n4. **Incremental Parsing**\n   - Time to reparse after small change\n   - Memory delta for incremental update\n   - Accuracy of incremental updates\n\n5. **Language Coverage**\n   - Test parsing in: Python, JavaScript, TypeScript, Go, Rust, Java, C++, Ruby\n   - Accuracy of AST for each language\n   - Error recovery quality\n\n**Special Considerations for Code Indexing**:\n\n1. **Multi-language support**: Critical, can't use language-specific only\n2. **Error recovery**: Real code has syntax errors, parser must handle\n3. **Speed**: Parsing is significant portion of indexing time\n4. **Incremental**: For file-level reindexing optimization\n5. **AST detail**: Need function/class/symbol extraction\n6. **Maintainability**: Grammar updates required as languages evolve\n\n**Deliverables**:\n\n1. **Research Report** (Markdown document)\n   - Executive summary with recommendation\n   - Detailed analysis for each parser\n   - Cited sources (minimum 30 total, 5-10 per major option)\n   - Decision matrix with weighted scores\n   - Language support comparison table\n\n2. **Benchmark Results**\n   - Parsing speed comparison (charts)\n   - Memory usage comparison\n   - AST quality comparison\n   - Reproducible benchmark scripts\n\n3. **Code Samples**\n   - Proof-of-concept implementations\n   - AST extraction examples\n   - Error handling demonstrations\n\n4. **Migration Assessment** (if switching)\n   - Code changes required\n   - Grammar/parser configuration\n   - Testing strategy\n   - Risk analysis\n\n5. **Recommendation**\n   - Stay with tree-sitter OR switch to X\n   - Evidence-based justification\n   - Hybrid approach if applicable (tree-sitter + language-specific for critical languages)\n\n**Timeline**: 1-2 weeks\n\n**Success Criteria**:\n- Minimum 30 total data sources\n- All benchmarks executed on Scout + large repo\n- AST quality verified for 5+ languages\n- Decision matrix completed with objective scoring\n- Clear recommendation with evidence\n\n**Open Questions**:\n- Can we use tree-sitter + language-specific parsers for critical languages (Python)?\n- What's the accuracy delta between tree-sitter and native parsers?\n- Is tree-sitter grammar maintenance active enough?\n- Do we need full AST or just symbol extraction?\n- Can LSP integration provide better parsing without reinventing?\n\n**Critical Factors**:\n- tree-sitter good enough for current needs\n- Multi-language support critical (can't maintain 10+ parsers)\n- Speed less critical than accuracy for code search\n- Error recovery important for real-world code\n- Community support and maintenance critical long-term\n\n**Next Steps After Research**:\n- If tree-sitter optimal: Document decision, continue usage\n- If alternative better: Create migration plan\n- If hybrid approach: Design integration architecture\n- If inconclusive: Extend research, gather more data","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-29T11:05:11.898968-05:00","updated_at":"2025-10-29T11:06:20.144571-05:00","dependencies":[{"issue_id":"mcpIndexer-148","depends_on_id":"mcpIndexer-145","type":"parent-child","created_at":"2025-10-29T11:05:22.735105-05:00","created_by":"gkatechis"}]}
{"id":"mcpIndexer-149","title":"Research: sentence-transformers vs alternative embedding frameworks","description":"**Objective**: Determine if sentence-transformers is the optimal embedding framework for Scout's code semantic search through comprehensive research and benchmarking.\n\n**Current State**:\n- sentence-transformers chosen for embedding generation\n- PyTorch-based, GPU-enabled\n- Using multi-qa-mpnet-base-dot-v1 model\n- Easy integration, model hub access\n\n**Research Question**:\nIs sentence-transformers the best choice for production embedding generation, or would an alternative framework provide better performance, cost, flexibility, or features?\n\n**Alternatives to Evaluate**:\n\n1. **Hugging Face Transformers (Direct)**\n   - Strengths: More control, broader ecosystem, latest models\n   - Concerns: More complex setup, need to implement pooling/normalization\n   - Use case: When sentence-transformers wrapper insufficient\n\n2. **OpenAI Embeddings API** (text-embedding-3-small/large)\n   - Strengths: State-of-art quality, no infrastructure, hosted\n   - Cost: $0.02-$0.13 per 1M tokens\n   - Concerns: API dependency, cost at scale, no offline\n   - Use case: Quality over cost, SaaS acceptable\n\n3. **Cohere Embeddings API** (embed-v3)\n   - Strengths: Competitive quality, cheaper than OpenAI\n   - Cost: $0.10 per 1M tokens\n   - Concerns: API dependency, vendor lock-in\n   - Use case: Alternative to OpenAI\n\n4. **ONNX Runtime**\n   - Strengths: Optimized inference, cross-platform, quantization\n   - Deployment: Convert model to ONNX, use onnxruntime\n   - Performance: 2-4x faster inference than PyTorch\n   - Use case: Production optimization\n\n5. **TensorFlow/Keras**\n   - Strengths: TensorFlow ecosystem, TFLite for mobile\n   - Concerns: Fewer sentence transformer models available\n   - Use case: If TensorFlow infrastructure already exists\n\n6. **FastEmbed (Qdrant)**\n   - Strengths: Optimized for speed, ONNX-based, lightweight\n   - Models: Subset of sentence-transformers models\n   - Use case: Speed-optimized inference\n\n7. **txtai**\n   - Strengths: All-in-one (embeddings + vector DB + pipelines)\n   - Concerns: Framework lock-in, less flexible\n   - Use case: Rapid prototyping\n\n8. **LangChain Embeddings**\n   - Strengths: Unified interface, many providers\n   - Concerns: Abstraction overhead, dependency on LangChain\n   - Use case: If using LangChain ecosystem\n\n9. **LlamaIndex Embeddings**\n   - Strengths: Unified interface, local + API models\n   - Concerns: Dependency on LlamaIndex\n   - Use case: If using LlamaIndex\n\n10. **Custom Inference Pipeline**\n    - Load PyTorch/ONNX model directly\n    - Implement batching, pooling, normalization\n    - Strengths: Maximum control, minimal dependencies\n    - Concerns: Reinventing wheel, maintenance burden\n\n11. **Voyage AI Embeddings**\n    - Strengths: Code-specific models, high quality\n    - Cost: Usage-based pricing\n    - Concerns: API dependency\n\n**Evaluation Criteria** (weighted):\n\n1. **Inference Performance (30%)**\n   - Embedding generation speed (embeddings/sec)\n   - Batch processing efficiency\n   - GPU utilization\n   - Memory usage\n   - Latency (single embedding)\n\n2. **Cost (25%)**\n   - Self-hosted compute costs\n   - API costs (if applicable)\n   - Storage costs\n   - Total cost at 1M, 10M, 100M embeddings\n\n3. **Model Ecosystem (20%)**\n   - Available models (quantity)\n   - Code-specific models availability\n   - Model quality (MTEB benchmarks)\n   - Custom model support\n   - Fine-tuning capabilities\n\n4. **Operational Characteristics (15%)**\n   - Deployment complexity\n   - Offline capability (critical for local-first)\n   - Dependencies (size, complexity)\n   - Monitoring/debugging\n   - Error handling\n\n5. **Developer Experience (10%)**\n   - API simplicity\n   - Documentation quality\n   - Community support\n   - Integration examples\n   - Update frequency\n\n**Data Sources Required (5-20 per framework)**:\n\n**Performance Evidence**:\n- [ ] Official benchmarks (inference speed)\n- [ ] Third-party performance comparisons\n- [ ] GPU vs CPU performance studies\n- [ ] Batch size optimization reports\n- [ ] Memory profiling studies\n- [ ] ONNX optimization benchmarks\n- [ ] Quantization impact analysis\n\n**Quality Evidence**:\n- [ ] MTEB (Massive Text Embedding Benchmark) scores\n- [ ] Code search specific benchmarks\n- [ ] Semantic similarity quality tests\n- [ ] Cross-lingual performance\n- [ ] Domain-specific performance (code)\n\n**Cost Evidence**:\n- [ ] API pricing documentation\n- [ ] Self-hosted cost analyses (AWS/GCP compute)\n- [ ] Total cost of ownership comparisons\n- [ ] Break-even analysis (self-hosted vs API)\n- [ ] Cost projections at scale\n\n**Production Evidence**:\n- [ ] Case studies using each framework\n- [ ] Production deployment stories\n- [ ] Reliability reports\n- [ ] Migration experiences\n- [ ] Maintenance burden assessments\n\n**Ecosystem Evidence**:\n- [ ] Model availability (HuggingFace hub, etc.)\n- [ ] GitHub activity\n- [ ] Community size\n- [ ] Integration examples\n- [ ] Framework adoption metrics\n\n**Specific Use Cases to Analyze**:\n- [ ] Sourcegraph (code search embedding approach)\n- [ ] GitHub Copilot (inference stack)\n- [ ] Cursor (code embedding approach)\n- [ ] Phind (code search)\n- [ ] Production RAG systems (LangChain vs custom)\n\n**Benchmarks to Conduct**:\n\n1. **Embed Scout Codebase (~300 chunks)**\n   - Time to generate all embeddings\n   - Memory usage\n   - GPU utilization\n   - Implementation complexity\n\n2. **Embed Large Repo (zendesk/zendesk ~50k chunks)**\n   - Time to embed (single batch)\n   - Time to embed (optimal batching)\n   - Memory peak\n   - Cost (if API)\n\n3. **Inference Speed Tests**\n   - Single embedding latency\n   - Batch sizes: 1, 8, 32, 64, 128, 256\n   - CPU vs GPU performance\n   - Quantized vs full precision\n\n4. **Quality Tests**\n   - Semantic similarity accuracy\n   - Code search relevance (query → relevant code)\n   - Compare embeddings across frameworks\n   - MTEB benchmark subset\n\n5. **Cost Comparison**\n   - Self-hosted: Scout codebase, large repo\n   - API: Same workloads, calculate costs\n   - Break-even points\n   - Ongoing costs (reindexing, updates)\n\n6. **Operational Tests**\n   - Offline capability verification\n   - Dependency installation\n   - Docker image size\n   - Cold start time\n   - Error handling\n\n**Special Considerations for Code Embeddings**:\n\n1. **Offline-first**: Critical for Scout's local-first design\n2. **GPU acceleration**: Important for large-scale indexing\n3. **Model flexibility**: Need to test multiple models\n4. **Batch efficiency**: Indexing involves large batches\n5. **Memory constraints**: Large models need careful management\n6. **Code-specific models**: Do code-trained models help?\n7. **Fine-tuning**: Can we improve with custom training?\n\n**Deliverables**:\n\n1. **Research Report** (Markdown document)\n   - Executive summary with recommendation\n   - Detailed analysis for each framework\n   - Cited sources (minimum 40 total, 5-10 per framework)\n   - Decision matrix with weighted scores\n   - MTEB scores comparison table\n   - Cost comparison table\n\n2. **Benchmark Results**\n   - Inference speed comparison (charts)\n   - Quality comparison (MTEB scores)\n   - Cost analysis (tables)\n   - Memory usage comparison\n   - Reproducible benchmark scripts\n\n3. **Cost Model**\n   - Total cost calculator (self-hosted vs API)\n   - Break-even analysis\n   - Scaling cost projections\n   - ROI analysis\n\n4. **Migration Assessment** (if switching)\n   - Code changes required\n   - Model conversion process\n   - Testing strategy\n   - Rollback plan\n   - Risk analysis\n\n5. **Recommendation**\n   - Stay with sentence-transformers OR switch to X\n   - Evidence-based justification\n   - Hybrid approach if applicable\n   - Timeline and resource requirements\n\n**Timeline**: 1.5-2 weeks\n\n**Success Criteria**:\n- Minimum 40 total data sources\n- All benchmarks executed on consistent hardware\n- MTEB scores compared across frameworks\n- Cost model completed for 3+ scale scenarios\n- Decision matrix completed with objective scoring\n- Clear recommendation with evidence\n\n**Open Questions**:\n- Is API cost justified by quality improvement?\n- Can ONNX optimization provide 2-4x speedup?\n- Are code-specific models worth the tradeoff?\n- Should we support multiple embedding providers?\n- Can we fine-tune for code search specific improvement?\n- What's the model update/versioning strategy?\n\n**Critical Factors**:\n- sentence-transformers sufficient for current needs\n- Offline capability critical for local-first design\n- Cost matters at scale (1M+ embeddings)\n- GPU acceleration important for large repos\n- Model flexibility important for experimentation\n- Community support critical for long-term\n\n**Key Trade-offs**:\n\n1. **Self-hosted vs API**\n   - Self-hosted: Control, offline, fixed cost\n   - API: Quality, simplicity, variable cost\n\n2. **PyTorch vs ONNX**\n   - PyTorch: Flexibility, model availability\n   - ONNX: Speed, deployment simplicity\n\n3. **Generic vs Code-specific**\n   - Generic: Broader ecosystem, proven\n   - Code-specific: Potentially better, less mature\n\n**Next Steps After Research**:\n- If sentence-transformers optimal: Document decision, optimize usage\n- If ONNX better: Create ONNX conversion pipeline\n- If API better: Create hybrid (API + local fallback)\n- If custom pipeline: Design and implement\n- If inconclusive: Extend research, gather more data","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-29T11:06:31.92064-05:00","updated_at":"2025-10-29T11:07:45.065008-05:00","dependencies":[{"issue_id":"mcpIndexer-149","depends_on_id":"mcpIndexer-145","type":"parent-child","created_at":"2025-10-29T11:06:42.405817-05:00","created_by":"gkatechis"}]}
{"id":"mcpIndexer-150","title":"Research: Chunking strategies for code semantic search","description":"","status":"open","priority":2,"issue_type":"task","created_at":"2025-10-29T12:38:18.797333-05:00","updated_at":"2025-10-29T12:38:18.797333-05:00"}
{"id":"mcpIndexer-151","title":"Research: n_results optimization and dynamic tuning","description":"","status":"open","priority":2,"issue_type":"task","created_at":"2025-10-29T12:38:31.534547-05:00","updated_at":"2025-10-29T12:38:31.534547-05:00"}
{"id":"mcpIndexer-152","title":"Research: Metadata filtering best practices for vector search","description":"","status":"open","priority":2,"issue_type":"task","created_at":"2025-10-29T12:38:36.638508-05:00","updated_at":"2025-10-29T12:38:36.638508-05:00"}
{"id":"mcpIndexer-153","title":"Research: Context building and prompt engineering for RAG","description":"","status":"open","priority":2,"issue_type":"task","created_at":"2025-10-29T12:38:41.728481-05:00","updated_at":"2025-10-29T12:38:41.728481-05:00"}
{"id":"mcpIndexer-154","title":"Research: Hybrid search implementations (semantic + keyword)","description":"**Objective**: Research and validate hybrid search implementations combining semantic vector search with keyword/BM25 search for improved code retrieval accuracy.\n\n**Context**: ChromaDB best practices recommend hybrid search (semantic + keyword) for production RAG systems. Scout currently only uses semantic search via sentence-transformers embeddings.\n\n**Previous Implementation Attempt** (feature/hybrid-search branch):\n- Implemented RRF (Reciprocal Rank Fusion) for combining results\n- Used rank-bm25 library for keyword search\n- Created benchmark demonstrating the approach\n\n**Critical Lessons Learned**:\n❌ **Tokenization is the blocker** - Simple whitespace splitting doesn't work for code:\n- Query 'authenticate_user' → No keyword results (not split into tokens)\n- Query 'hash_password bcrypt' → No keyword results (symbols with underscores/parens not tokenized)\n- BM25 is fast (\u003c0.1ms) but useless without proper code tokenization\n\n✅ **What worked**:\n- RRF fusion algorithm (k=60) successfully combines ranked lists\n- Parallel execution of semantic + keyword search\n- Hybrid search was faster than pure semantic (~15ms vs ~50ms)\n\n**Key Research Questions**:\n\n1. **Code-Aware Tokenization** (CRITICAL)\n   - How do production code search tools tokenize?\n   - snake_case splitting: 'authenticate_user' → ['authenticate', 'user']\n   - CamelCase splitting: 'getUserName' → ['get', 'user', 'name']\n   - Special char handling: 'user.authenticate()' → ['user', 'authenticate']\n   - Should we preserve original tokens too? ('authenticate_user' + parts)\n\n2. **Hybrid Search Implementations**\n   - RRF vs alpha weighting - which is better for code?\n   - How does Sourcegraph/GitHub code search tokenize?\n   - What does Elasticsearch/OpenSearch use for code?\n   - ChromaDB native hybrid search vs custom implementation?\n\n3. **Keyword Index Strategy**\n   - Separate BM25 index vs ChromaDB documents?\n   - Index granularity (per-chunk vs per-symbol)?\n   - Memory/disk tradeoffs?\n\n4. **Query Processing**\n   - Should queries be tokenized same as code?\n   - Natural language queries vs code symbol queries?\n   - Multi-query strategies (run multiple variants)?\n\n**Data Sources Required** (10-20 total):\n\n**Production Code Search Evidence**:\n- [ ] Sourcegraph tokenization approach\n- [ ] GitHub code navigation/search implementation\n- [ ] Elasticsearch code analysis tokenizer\n- [ ] OpenGrok/OpenSearch for code\n- [ ] JetBrains IDE indexing strategy\n- [ ] VS Code search implementation\n- [ ] grep/ripgrep vs BM25 for code\n\n**Academic/Research Evidence**:\n- [ ] Papers on code search and IR for source code\n- [ ] Tokenization strategies for programming languages\n- [ ] Hybrid search benchmarks (semantic + keyword)\n- [ ] RRF vs other fusion algorithms\n\n**Library/Framework Evidence**:\n- [ ] rank-bm25 documentation and limitations\n- [ ] Whoosh (Python full-text search) for code\n- [ ] LlamaIndex hybrid retriever implementation\n- [ ] LangChain ensemble retriever\n- [ ] ChromaDB upcoming hybrid search feature\n\n**Benchmarks to Conduct**:\n\n1. **Tokenization Comparison**\n   - Simple whitespace splitting (baseline - we know this fails)\n   - Regex-based code tokenization (snake_case, CamelCase)\n   - Tree-sitter token extraction\n   - Language-specific tokenizers (Python tokenize module)\n   - Measure: precision/recall on exact function names\n\n2. **Hybrid Search Quality**\n   - Pure semantic search (baseline)\n   - Pure keyword search (with good tokenization)\n   - Hybrid RRF (k=60)\n   - Hybrid alpha-weighted (α=0.3, 0.5, 0.7)\n   - Measure: relevance, speed, recall@10\n\n3. **Query Types**\n   - Exact function names: 'authenticate_user'\n   - Partial matches: 'user authenticate'\n   - Natural language: 'how does authentication work'\n   - Multi-term code: 'hash_password bcrypt'\n   - Measure: which search type works best for each\n\n4. **Performance Benchmarks**\n   - Indexing time with keyword index\n   - Query latency (semantic vs hybrid)\n   - Memory usage of dual indexes\n   - Scale test on zendesk/zendesk repo\n\n**Deliverables**:\n\n1. **Research Report** (Markdown)\n   - Executive summary with recommendation\n   - Tokenization strategy comparison (with examples)\n   - Production code search analysis\n   - Cited sources (10-20 total)\n   - Decision: Should we implement hybrid search?\n\n2. **Tokenization Proof-of-Concept**\n   - Implement 3-5 tokenization strategies\n   - Test on real code samples\n   - Precision/recall metrics\n   - Code examples showing what works\n\n3. **Benchmark Results**\n   - Comparison charts (semantic vs keyword vs hybrid)\n   - Query-type performance matrix\n   - Cost/benefit analysis\n   - Reproducible benchmark script\n\n4. **Implementation Plan** (if recommended)\n   - Architecture design (where tokenizer fits)\n   - Code changes required\n   - Migration strategy\n   - Testing approach\n   - Timeline estimate\n\n**Success Criteria**:\n- Minimum 10 data sources cited\n- Tokenization strategy validated on real code\n- Hybrid search benchmarked vs current semantic search\n- Clear go/no-go recommendation\n- If go: implementation plan with timeline\n- If no-go: evidence why semantic-only is sufficient\n\n**Open Questions**:\n- Is the tokenization complexity worth the improvement?\n- Can we use tree-sitter for tokenization (already have AST)?\n- Should we implement keyword search at all, or improve semantic?\n- Can we use grep/ripgrep instead of BM25?\n- Is ChromaDB planning native hybrid search support?\n\n**Critical Factors**:\n- Tokenization is make-or-break for keyword search on code\n- Hybrid search is fast but only if keyword search works\n- Code search is different from document search\n- Production tools likely have sophisticated tokenization\n\n**Timeline**: 1-2 weeks\n\n**Related Issues**:\n- mcpIndexer-128: Previous hybrid search implementation (closed, tokenization issues)\n- mcpIndexer-130: Tokenization improvement task (closed)\n- See feature/hybrid-search branch for previous implementation\n\n**Next Steps After Research**:\n- If tokenization solvable: Implement hybrid search with code-aware tokenization\n- If tokenization too complex: Document why, focus on semantic improvements\n- If unclear: Extend research, try more approaches","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-29T12:38:46.818271-05:00","updated_at":"2025-10-29T12:49:06.056265-05:00"}
{"id":"mcpIndexer-155","title":"Research: Query expansion and rewriting techniques","description":"","status":"open","priority":2,"issue_type":"task","created_at":"2025-10-29T12:38:51.900995-05:00","updated_at":"2025-10-29T12:38:51.900995-05:00"}
{"id":"mcpIndexer-156","title":"Research: Re-ranking algorithms for improved retrieval","description":"","status":"open","priority":2,"issue_type":"task","created_at":"2025-10-29T12:39:03.675337-05:00","updated_at":"2025-10-29T12:39:03.675337-05:00"}
{"id":"mcpIndexer-157","title":"Research: Contextual compression and result summarization","description":"","status":"open","priority":2,"issue_type":"task","created_at":"2025-10-29T12:39:08.783741-05:00","updated_at":"2025-10-29T12:39:08.783741-05:00"}
{"id":"mcpIndexer-158","title":"Research: MMR (Maximal Marginal Relevance) for diverse results","description":"","status":"open","priority":2,"issue_type":"task","created_at":"2025-10-29T12:39:13.889575-05:00","updated_at":"2025-10-29T12:39:13.889575-05:00"}
{"id":"mcpIndexer-68","title":"No dependency lock file (requirements.lock or poetry.lock)","description":"**Context**: Scout now uses `pyproject.toml` as the single source of truth for dependencies. The legacy `requirements.txt` has been removed. Dependencies are installed via `pip install -e \".[dev]\"`.\n\n**Problem**: Without a lock file, different developers/deployments may get different dependency versions, leading to:\n- \"Works on my machine\" issues\n- Unexpected behavior from dependency updates\n- Difficult-to-reproduce bugs\n- Non-deterministic CI/CD builds\n\n**Current State**:\n- ✅ Dependencies defined in `pyproject.toml`\n- ✅ No redundant `requirements.txt`\n- ❌ No lock file - versions can drift\n- ❌ No reproducible builds\n\n**Lock File Options**:\n\n### 1. pip-tools (Recommended)\n```bash\n# Generate lock file\npip-compile pyproject.toml -o requirements.lock\n\n# Install from lock file\npip-sync requirements.lock\n\n# Update specific package\npip-compile --upgrade-package chromadb pyproject.toml\n```\n\n**Pros**:\n- Works with standard pip\n- Generates from pyproject.toml\n- Supports hash verification\n- Simple, widely used\n\n**Cons**:\n- Separate tool to install\n- Manual lock file generation\n\n### 2. Poetry\n```bash\npoetry lock        # Generate poetry.lock\npoetry install     # Install from lock\npoetry update      # Update dependencies\n```\n\n**Pros**:\n- Full dependency management\n- Virtual env management\n- Publishing support\n- Mature, popular\n\n**Cons**:\n- Different package manager\n- Requires migration from pip\n- More complex\n\n### 3. uv (Newest)\n```bash\nuv pip compile pyproject.toml -o uv.lock\nuv pip sync uv.lock\n```\n\n**Pros**:\n- Extremely fast (Rust-based)\n- Compatible with pip\n- Growing adoption\n\n**Cons**:\n- Newer, less proven\n- Smaller ecosystem\n\n**Priority**: LOW (3) - Not critical for current development stage\n\n**When to Implement**:\n- Before production deployments\n- When team grows (\u003e3 developers)\n- After core features stabilized\n- If reproducibility issues arise\n\n**Recommendation**: Use **pip-tools** when ready - minimal disruption, works with existing pip workflow.\n\n**Implementation Checklist** (when ready):\n- [ ] Choose lock file tool (pip-tools recommended)\n- [ ] Generate lock file from pyproject.toml\n- [ ] Update setup.sh to use lock file\n- [ ] Update CI/CD to use lock file\n- [ ] Document lock file workflow in docs\n- [ ] Add lock file to version control\n- [ ] Document how to update dependencies","status":"open","priority":3,"issue_type":"task","created_at":"2025-10-27T14:20:48.074114-05:00","updated_at":"2025-10-29T13:03:08.683483-05:00","dependencies":[{"issue_id":"mcpIndexer-68","depends_on_id":"mcpIndexer-52","type":"discovered-from","created_at":"2025-10-27T14:20:48.075351-05:00","created_by":"gkatechis"}]}
